{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filepath = \"C:\\\\Users\\\\padhu\\\\Documents\\\\Spring 2016\\\\MSA 8050 Unstructured data Management\\\\Final project\\\\DATA.csv\"\n",
    "\n",
    "df = pd.DataFrame.from_csv(filepath,index_col = None)\n",
    "df1=df\n",
    "useless_tags = ['CC','CS','DTX','PN$','PP$','PP$$','PPL','PPLS','PPO','PPS','PPSS','PRP','PRP$','RP','UH','WP$','WPO','WPS','AP',\n",
    "'DT','DTI','DTS','IN','MD','WDT']\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "import nltk.tag\n",
    "from nltk import tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from __future__ import division\n",
    "\n",
    "infile = open(\"C:\\\\Users\\\\padhu\\\\Documents\\\\Spring 2016\\\\MSA 8050 Unstructured data Management\\\\Final project\\\\Apple_2014(cleaned).txt\")\n",
    "file_str = infile.read()\n",
    "text=nltk.word_tokenize(file_str)\n",
    "\n",
    "prev = len(text)\n",
    "tags = pos_tag(text)\n",
    "tag_dict = dict()\n",
    "\n",
    "for tag in tags:\n",
    "    if tag[1] in tag_dict.keys():\n",
    "        tag_dict[tag[1]].append(tag[0])\n",
    "    else:\n",
    "        tag_dict[tag[1]] = [tag[0]]\n",
    "#clear punctuations from text\n",
    "text = [item.translate(None, string.punctuation) for item in text]\n",
    "text = [item for item in text if len(item)>0]\n",
    "\n",
    "useless_words = [ tag_dict[item] for item in tag_dict.keys() if item in useless_tags]\n",
    "useless_words = [list(set(item)) for item in useless_words]\n",
    "useless_words = sum(useless_words,[])\n",
    "useless_words = list(set(useless_words))\n",
    "\n",
    "temp1 = str(df1[df1['Company']=='Apple']['10KText'])\n",
    "\n",
    "def remove_uselss_words(text_part):\n",
    "    text_part_list = text_part.split(' ')\n",
    "    text_part_list = [item for item in text_part_list if item not in useless_words]\n",
    "    return ' '.join(text_part_list)\n",
    "\n",
    "text10k = df['10KText']\n",
    "res_list = []\n",
    "for item in text10k:\n",
    "    res_list.append(remove_uselss_words(item))\n",
    "\n",
    "df['10KText'] = res_list\n",
    "\n",
    "temp = str(df[df['Company']=='Apple']['10KText'])\n",
    "len(temp.split(' '))\n",
    "len(remove_uselss_words(temp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cols = ['Company','Year','10KText','Diversity','Complexity','Quantity', 'Avg_Word_Length','Target']\n",
    "df = pd.DataFrame(index = None,columns =cols)\n",
    "\n",
    "path = \"C:\\\\Users\\\\padhu\\\\Documents\\\\Spring 2016\\\\MSA 8050 Unstructured data Management\\\\Final project\\\\text_files\\\\\"\n",
    "dirs = os.listdir(path)\n",
    "fraud_ind =0\n",
    "for item in dirs:\n",
    "    if item == 'Fraud':\n",
    "        fraud_ind =1\n",
    "    sub_path = os.listdir(path+str(item))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "import nltk.tag\n",
    "from nltk import tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_metrics(text_10k):\n",
    "    file_str = text_10k\n",
    "    text=nltk.word_tokenize(file_str)\n",
    "    tags = pos_tag(text)\n",
    "    tag_dict = dict()\n",
    "\n",
    "    for tag in tags:\n",
    "        if tag[1] in tag_dict.keys():\n",
    "            tag_dict[tag[1]].append(tag[0])\n",
    "        else:\n",
    "            tag_dict[tag[1]] = [tag[0]]\n",
    "    #clear punctuations from text\n",
    "    text = [item.translate(None, string.punctuation) for item in text]\n",
    "    text = [item for item in text if len(item)>0]\n",
    "    \n",
    "    func_word_tags = ['AT', 'CC','CS','DTX','PN','PN$','PP$','PP$$','PPL','PPLS','PPO','PPS','PPSS','PRP','PRP$','RP','UH','WP$',\n",
    "                      'WPO', 'WPS']\n",
    "    content_word_tags = ['ABL','ABN','ABX','AP','BE','BED','BEDZ','BEG','BEM','BEN','BER','BEZ','CD','DO','DOD','DOZ','DT','DTI',\n",
    "                         'DTS','EX','FW','HV','HVD','HVG','HVN','IN','JJ','JJR','JJS','JJT','MD','NC','NN','NN$',\n",
    "                         'NNS','NNS$','NP','NP$','NPS','NPS$','NR','OD','QL','QLP','RB','RBR','RBT','RN',\n",
    "                         'TO','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WQL','WRB']\n",
    "\n",
    "    func_words = []\n",
    "    content_words = [] \n",
    "    for item in tag_dict.keys():\n",
    "        if item in func_word_tags:\n",
    "            func_words.extend(tag_dict[item])\n",
    "        elif item in content_word_tags:\n",
    "            content_words.extend(tag_dict[item])\n",
    "\n",
    "    #Metrics Calculation\n",
    "    content_word_diversity = len(set(content_words))/len(content_words)\n",
    "    func_word_diversity = len(set(func_words))/len(func_words)\n",
    "    lexical_diversity = len(set(text))/len(text)\n",
    "    diversity = (lexical_diversity+content_word_diversity)/2\n",
    "    \n",
    "    ##Average sentence length\n",
    "    sentences = file_str.split('.')\n",
    "    #sentences =[item.split(' ') for item in sentences]\n",
    "    sentences_lengths = [len(item.split(' ')) for item in sentences]\n",
    "    avg_sentence_length = sum(sentences_lengths)/len(sentences_lengths)\n",
    "\n",
    "    synt_complexity = (func_word_diversity+avg_sentence_length)/2\n",
    "\n",
    "    def syllables(word):\n",
    "        count = 0\n",
    "        vowels = 'aeiouy'\n",
    "        word = word.lower().strip(\".:;?!\")\n",
    "        if word[0] in vowels:\n",
    "            count +=1\n",
    "        for index in range(1,len(word)):\n",
    "            if word[index] in vowels and word[index-1] not in vowels:\n",
    "                count +=1\n",
    "        if word.endswith('e'):\n",
    "            count -= 1\n",
    "        if word.endswith('le'):\n",
    "            count+=1\n",
    "        if count == 0:\n",
    "            count +=1\n",
    "        return count\n",
    "\n",
    "    ##Average word length\n",
    "    word_lengths = [syllables(item) for item in text]\n",
    "    avg_word_length = sum(word_lengths)/len(word_lengths)\n",
    "\n",
    "    verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    verb_count = [len(tag_dict[item]) for item in tag_dict.keys() if item in verb_tags]\n",
    "    total_verbs = sum(verb_count)\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = len(text)\n",
    "    \n",
    "    quantity = (total_verbs+total_sentences+total_words)/3\n",
    "    \n",
    "    ##Sentory Terms Calculation\n",
    "    sensory_words = ['bleary','blurred','brilliant','colorless','dazzling','dim','dingy','faded','faint','flashy','gaudy','glance',\n",
    "'gleaming','glimpse','glistening','glittering','gloomy','glossy','glowing','grimy','hazy','indistinct','misty','peer','radiant',\n",
    "'shadowy','shimmering','shiny','smudged','sparkling','streaked','striped','tarnished','twinkling','bellow','blare','buzz','cackle',\n",
    "'cheer','clamor','clang','crackle','creak','grumble','gurgle','hiss','howl','hush','jabber','mumble','murmur','mutter','rant',\n",
    "'rave','roar','rumble','rustle','screech','shriek','shrill','sizzle','snarl','squawk','squeal','swish','thud','thump','whimper',\n",
    "'yelp','balmy','biting','bristly','bumpy','chilly','coarse','cold','cool','crawly','creepy','cuddly','dusty','feathery','feverish',\n",
    "'fluffy','furry','fuzzy','gooey','greasy','gritty','hairy','hot','icy','limp','lumpy','moist','oily','powdery','prickly','scratchy',\n",
    "'shivery','silky','slimy','slippery','spongy','springy','squashy','sticky','sweaty','velvety','appetizing','bitter','bland','creamy',\n",
    "'delectable','delicious','flavorful','flavorless','gingery','luscious','nauseating','palatable','peppery','piquant','refreshing',\n",
    "'ripe','rotten','salty','savory','scrumptious','sharp','sour','spicy','spoiled','stale','sugary','sweet','tangy','tasteless',\n",
    "'tasty','unappetizing','unripe','vinegary','yummy','zesty','acrid','aroma','aromatic','fetid','foul-smelling','fragrant','moldy',\n",
    "'musty','nidorous','odiferous','odor','odorless','old','perfumed','pungent','putrid','rancid','rank','reeking','scent','scented',\n",
    "'smell','spicy','steno','sweet','waft','whiff']\n",
    "    #Sensory Ratio\n",
    "    stemmer1 = SnowballStemmer(\"english\")\n",
    "\n",
    "    sensory_words_stemmed = [stemmer1.stem(item) for item in sensory_words]\n",
    "    text_unicode_trimmed = [s.decode('unicode_escape').encode('ascii','ignore') for s in text]\n",
    "    text_stemmed = [stemmer1.stem(item) for item in text_unicode_trimmed]\n",
    "    sensory_words_count=0\n",
    "    for item in text_stemmed:\n",
    "        if item in sensory_words_stemmed or item in sensory_words:\n",
    "            sensory_words_count+=1\n",
    "\n",
    "    sensory_ratio = sensory_words_count/len(text)\n",
    "    \n",
    "    return [diversity,synt_complexity,quantity,avg_word_length,sensory_ratio]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28936207284172666, 11.535930551613209, 1375.0, 1.8854676258992806, 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = open(\"C:\\\\Users\\\\padhu\\\\Documents\\\\Spring 2016\\\\MSA 8050 Unstructured data Management\\\\Final project\\\\Boeing_MDA.txt\")\n",
    "file_str = infile.read()\n",
    "text_metrics(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = open(\"C:\\\\Users\\\\padhu\\\\Documents\\\\Spring 2016\\\\MSA 8050 Unstructured data Management\\\\Final project\\\\Boeing_MDA.txt\")\n",
    "file_str = infile.read()\n",
    "text=nltk.word_tokenize(file_str)\n",
    "tags = pos_tag(text)\n",
    "tag_dict = dict()\n",
    "\n",
    "for tag in tags:\n",
    "    if tag[1] in tag_dict.keys():\n",
    "        tag_dict[tag[1]].append(tag[0])\n",
    "    else:\n",
    "        tag_dict[tag[1]] = [tag[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clear punctuations from text\n",
    "import string\n",
    "text = [item.translate(None, string.punctuation) for item in text]\n",
    "text = [item for item in text if len(item)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Diversity Metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29296875"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_word_tags = ['AT', 'CC','CS','DTX','PN','PN$','PP$','PP$$','PPL','PPLS','PPO','PPS','PPSS','PRP','PRP$','RP','UH','WP$',\n",
    "'WPO', 'WPS']\n",
    "content_word_tags = ['ABL','ABN','ABX','AP','BE','BED','BEDZ','BEG','BEM','BEN','BER','BEZ','CD','DO','DOD','DOZ','DT','DTI',\n",
    "'DTS','EX','FW','HV','HVD','HVG','HVN','IN','JJ','JJR','JJS','JJT','MD','NC','NN','NN$','NNS','NNS$','NP','NP$','NPS','NPS$',\n",
    "'NR','OD','QL','QLP','RB','RBR','RBT','RN','TO','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WQL','WRB']\n",
    "\n",
    "func_words = []\n",
    "content_words = [] \n",
    "for item in tag_dict.keys():\n",
    "    if item in func_word_tags:\n",
    "        func_words.extend(tag_dict[item])\n",
    "    elif item in content_word_tags:\n",
    "        content_words.extend(tag_dict[item])\n",
    "\n",
    "#Metrics Calculation\n",
    "content_word_diversity = len(set(content_words))/len(content_words)\n",
    "func_word_diversity = len(set(func_words))/len(func_words)\n",
    "lexical_diversity = len(set(text))/len(text)\n",
    "\n",
    "len(set(content_words))/len(content_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Syntactic Complexity__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Average sentence length\n",
    "sentences = file_str.split('.')\n",
    "#sentences =[item.split(' ') for item in sentences]\n",
    "sentences_lengths = [len(item.split(' ')) for item in sentences]\n",
    "avg_sentence_length = sum(sentences_lengths)/len(sentences_lengths)\n",
    "\n",
    "synt_complexity = (func_word_diversity+avg_sentence_length)/2\n",
    "\n",
    "def syllables(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower().strip(\".:;?!\")\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "##Average word length\n",
    "word_lengths = [syllables(item) for item in text]\n",
    "avg_word_length = sum(word_lengths)/len(word_lengths)\n",
    "\n",
    "verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "verb_count = [len(tag_dict[item]) for item in tag_dict.keys() if item in verb_tags]\n",
    "total_verbs = sum(verb_count)\n",
    "total_sentences = len(sentences)\n",
    "total_words = len(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sensory_words = ['bleary','blurred','brilliant','colorless','dazzling','dim','dingy','faded','faint','flashy','gaudy','glance',\n",
    "'gleaming','glimpse','glistening','glittering','gloomy','glossy','glowing','grimy','hazy','indistinct','misty','peer','radiant',\n",
    "'shadowy','shimmering','shiny','smudged','sparkling','streaked','striped','tarnished','twinkling','bellow','blare','buzz','cackle',\n",
    "'cheer','clamor','clang','crackle','creak','grumble','gurgle','hiss','howl','hush','jabber','mumble','murmur','mutter','rant',\n",
    "'rave','roar','rumble','rustle','screech','shriek','shrill','sizzle','snarl','squawk','squeal','swish','thud','thump','whimper',\n",
    "'yelp','balmy','biting','bristly','bumpy','chilly','coarse','cold','cool','crawly','creepy','cuddly','dusty','feathery','feverish',\n",
    "'fluffy','furry','fuzzy','gooey','greasy','gritty','hairy','hot','icy','limp','lumpy','moist','oily','powdery','prickly','scratchy',\n",
    "'shivery','silky','slimy','slippery','spongy','springy','squashy','sticky','sweaty','velvety','appetizing','bitter','bland','creamy',\n",
    "'delectable','delicious','flavorful','flavorless','gingery','luscious','nauseating','palatable','peppery','piquant','refreshing',\n",
    "'ripe','rotten','salty','savory','scrumptious','sharp','sour','spicy','spoiled','stale','sugary','sweet','tangy','tasteless',\n",
    "'tasty','unappetizing','unripe','vinegary','yummy','zesty','acrid','aroma','aromatic','fetid','foul-smelling','fragrant','moldy',\n",
    "'musty','nidorous','odiferous','odor','odorless','old','perfumed','pungent','putrid','rancid','rank','reeking','scent','scented',\n",
    "'smell','spicy','steno','sweet','waft','whiff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sensory Ratio\n",
    "stemmer1 = SnowballStemmer(\"english\")\n",
    "\n",
    "sensory_words_stemmed = [stemmer1.stem(item) for item in sensory_words]\n",
    "text_unicode_trimmed = [s.decode('unicode_escape').encode('ascii','ignore') for s in text]\n",
    "text_stemmed = [stemmer1.stem(item) for item in text_unicode_trimmed]\n",
    "sensory_words_count=0\n",
    "for item in text_stemmed:\n",
    "    if item in sensory_words_stemmed or item in sensory_words:\n",
    "        sensory_words_count+=1\n",
    "\n",
    "sensory_ratio = sensory_words_count/len(text)\n",
    "sensory_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
